{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import feature_column\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_size=1000\n",
    "test_frac = 0.1\n",
    "input_size = 18\n",
    "output_size = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input_size: 2 sets of 8 subjects + 2 indicators for activities\n",
    "output_size: 8 subject grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoremaker():\n",
    "    #empty scores represent the subjects that the student doesn't take\n",
    "    emptyScores = np.random.randint(low = 0, high = output_size, size=(3))        \n",
    "    \n",
    "    #randomly generate 2 sets of results. For different schools, scale the result accordingly\n",
    "    scores = np.random.randint(low = 50, high = 90, size=(output_size))\n",
    "    scores = scores + np.random.randint(low = -10, high = 10, size=(output_size))\n",
    "    scores2 = np.random.randint(low = 50, high = 90, size=(output_size))\n",
    "    scores2 = scores2 + np.random.randint(low = -10, high = 10, size=(output_size))\n",
    "    \n",
    "    #generate 2 values, 1 for humanities-related activities and one for science activities. More activities = higher score\n",
    "    activities = np.random.randint(low = 0, high = 10, size = (2))\n",
    "    \n",
    "    #generate fake labels for each listing.\n",
    "    scoreLabels = (scores + scores2) / 2\n",
    "    \n",
    "    #add activity score to labels for each subject\n",
    "    for i in range(output_size):\n",
    "        if(i < 4): scoreLabels[i] += activities[0]\n",
    "        else: scoreLabels[i] += activities[1]\n",
    "\n",
    "    scoreLabels = (scoreLabels - 40) / 60\n",
    "    \n",
    "    #make subjects empty\n",
    "    for i in emptyScores:\n",
    "        scores[i] = 0\n",
    "        scores2[i] = 0\n",
    "        scoreLabels[i] = 0\n",
    "        \n",
    "    return np.concatenate((scores, scores2, activities, scoreLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([92.        , 69.        , 72.        , 65.        ,  0.        ,\n",
       "        0.        , 73.        ,  0.        , 88.        , 74.        ,\n",
       "       58.        , 65.        ,  0.        ,  0.        , 83.        ,\n",
       "        0.        ,  6.        ,  2.        ,  0.93333333,  0.625     ,\n",
       "        0.51666667,  0.51666667,  0.        ,  0.        ,  0.66666667,\n",
       "        0.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoremaker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = np.array([scoremaker() for i in range(test_data_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[41.          0.         69.         55.         62.         83.\n",
      "   0.         45.         70.          0.         82.         88.\n",
      "  62.         83.          0.         68.          7.          6.\n",
      "   0.375       0.          0.70833333  0.64166667  0.46666667  0.81666667\n",
      "   0.          0.375     ]\n",
      " [50.         89.          0.         75.         57.          0.\n",
      "   0.         61.         79.         94.          0.         54.\n",
      "  73.          0.          0.         86.          5.          3.\n",
      "   0.49166667  0.94166667  0.          0.49166667  0.46666667  0.\n",
      "   0.          0.60833333]\n",
      " [84.         70.          0.         64.         70.          0.\n",
      "  76.         88.         90.         46.          0.         82.\n",
      "  61.          0.         89.         47.          8.          5.\n",
      "   0.91666667  0.43333333  0.          0.68333333  0.50833333  0.\n",
      "   0.79166667  0.54166667]\n",
      " [ 0.         86.         52.          0.         77.          0.\n",
      "  84.         84.          0.         76.         75.          0.\n",
      "  62.          0.         61.         60.          9.          1.\n",
      "   0.          0.83333333  0.54166667  0.          0.50833333  0.\n",
      "   0.55833333  0.55      ]\n",
      " [67.         48.         85.         81.          0.         83.\n",
      "   0.          0.         74.         69.         82.         68.\n",
      "   0.         93.          0.          0.          1.          5.\n",
      "   0.525       0.325       0.74166667  0.59166667  0.          0.88333333\n",
      "   0.          0.        ]\n",
      " [ 0.          0.         75.         72.          0.         64.\n",
      "  71.         74.          0.          0.         75.         54.\n",
      "   0.         62.         63.         57.          4.          3.\n",
      "   0.          0.          0.65        0.45        0.          0.43333333\n",
      "   0.5         0.475     ]\n",
      " [ 0.          0.         49.         64.         73.          0.\n",
      "  71.         96.          0.          0.         65.         93.\n",
      "  86.          0.         54.         75.          9.          1.\n",
      "   0.          0.          0.43333333  0.79166667  0.675       0.\n",
      "   0.39166667  0.775     ]\n",
      " [ 0.          0.         56.         68.         93.         52.\n",
      "  75.          0.          0.          0.         87.         51.\n",
      "  77.         77.         88.          0.          4.          3.\n",
      "   0.          0.          0.59166667  0.39166667  0.8         0.45833333\n",
      "   0.74166667  0.        ]\n",
      " [89.         78.         77.          0.          0.          0.\n",
      "  61.         53.         52.         70.         59.          0.\n",
      "   0.          0.         97.         71.          3.          1.\n",
      "   0.55833333  0.61666667  0.51666667  0.          0.          0.\n",
      "   0.66666667  0.38333333]\n",
      " [76.         77.         69.         57.          0.         84.\n",
      "   0.         69.         90.         91.         71.         72.\n",
      "   0.         65.          0.         89.          0.          9.\n",
      "   0.71666667  0.73333333  0.5         0.40833333  0.          0.725\n",
      "   0.          0.8       ]]\n"
     ]
    }
   ],
   "source": [
    "print(my_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = keras.Input(shape=(input_size,), name=\"subjects\")\n",
    "# x = layers.Dense(20,activation='relu', name='Hidden1') (inputs)\n",
    "# x = layers.Dense(20,activation='relu', name='Hidden2') (x)\n",
    "# outputs = layers.Dense(output_size, activation=\"relu\", name=\"predictions\")(x)\n",
    "# model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(input_size,), name=\"inputs\"))\n",
    "model.add(layers.Dense(20, activation='relu'))\n",
    "model.add(layers.Dense(output_size, activation=\"relu\", name=\"predictions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = my_data[:,:input_size]\n",
    "\n",
    "x_train = (x_train - np.average(x_train, axis = 0))/np.std(x_train, axis = 0)\n",
    "\n",
    "y_train = my_data[:,input_size:]\n",
    "x_val = x_train[(int)((1-test_frac)*test_data_size):]\n",
    "y_val = y_train[(int)((1-test_frac)*test_data_size):]\n",
    "x_train = x_train[:(int)((1-test_frac)*test_data_size)]\n",
    "y_train = y_train[:(int)((1-test_frac)*test_data_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(),\n",
    "    loss = keras.losses.mean_squared_error,\n",
    "    metrics = [keras.metrics.mean_squared_error]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit model on training data\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.2872 - mean_squared_error: 0.2872 - val_loss: 0.2668 - val_mean_squared_error: 0.2668\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.2167 - mean_squared_error: 0.2167 - val_loss: 0.2058 - val_mean_squared_error: 0.2058\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1783 - mean_squared_error: 0.1783 - val_loss: 0.1710 - val_mean_squared_error: 0.1710\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1545 - mean_squared_error: 0.1545 - val_loss: 0.1481 - val_mean_squared_error: 0.1481\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1381 - mean_squared_error: 0.1381 - val_loss: 0.1309 - val_mean_squared_error: 0.1309\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1249 - mean_squared_error: 0.1249 - val_loss: 0.1182 - val_mean_squared_error: 0.1182\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.1146 - mean_squared_error: 0.1146 - val_loss: 0.1073 - val_mean_squared_error: 0.1073\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.1055 - mean_squared_error: 0.1055 - val_loss: 0.0982 - val_mean_squared_error: 0.0982\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0973 - mean_squared_error: 0.0973 - val_loss: 0.0900 - val_mean_squared_error: 0.0900\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0896 - mean_squared_error: 0.0896 - val_loss: 0.0830 - val_mean_squared_error: 0.0830\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0825 - mean_squared_error: 0.0825 - val_loss: 0.0766 - val_mean_squared_error: 0.0766\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0760 - mean_squared_error: 0.0760 - val_loss: 0.0712 - val_mean_squared_error: 0.0712\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0703 - mean_squared_error: 0.0703 - val_loss: 0.0664 - val_mean_squared_error: 0.0664\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0649 - mean_squared_error: 0.0649 - val_loss: 0.0616 - val_mean_squared_error: 0.0616\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0594 - mean_squared_error: 0.0594 - val_loss: 0.0568 - val_mean_squared_error: 0.0568\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0545 - mean_squared_error: 0.0545 - val_loss: 0.0526 - val_mean_squared_error: 0.0526\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0501 - mean_squared_error: 0.0501 - val_loss: 0.0489 - val_mean_squared_error: 0.0489\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0465 - mean_squared_error: 0.0465 - val_loss: 0.0459 - val_mean_squared_error: 0.0459\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0433 - mean_squared_error: 0.0433 - val_loss: 0.0433 - val_mean_squared_error: 0.0433\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0401 - mean_squared_error: 0.0401 - val_loss: 0.0408 - val_mean_squared_error: 0.0408\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0375 - mean_squared_error: 0.0375 - val_loss: 0.0385 - val_mean_squared_error: 0.0385\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0352 - mean_squared_error: 0.0352 - val_loss: 0.0367 - val_mean_squared_error: 0.0367\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0331 - mean_squared_error: 0.0331 - val_loss: 0.0349 - val_mean_squared_error: 0.0349\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0314 - mean_squared_error: 0.0314 - val_loss: 0.0334 - val_mean_squared_error: 0.0334\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0298 - mean_squared_error: 0.0298 - val_loss: 0.0320 - val_mean_squared_error: 0.0320\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0284 - mean_squared_error: 0.0284 - val_loss: 0.0307 - val_mean_squared_error: 0.0307\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0271 - mean_squared_error: 0.0271 - val_loss: 0.0293 - val_mean_squared_error: 0.0293\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0282 - val_mean_squared_error: 0.0282\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0271 - val_mean_squared_error: 0.0271\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0238 - mean_squared_error: 0.0238 - val_loss: 0.0262 - val_mean_squared_error: 0.0262\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0255 - val_mean_squared_error: 0.0255\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0247 - val_mean_squared_error: 0.0247\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0238 - val_mean_squared_error: 0.0238\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0203 - mean_squared_error: 0.0203 - val_loss: 0.0230 - val_mean_squared_error: 0.0230\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0196 - mean_squared_error: 0.0196 - val_loss: 0.0223 - val_mean_squared_error: 0.0223\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0189 - mean_squared_error: 0.0189 - val_loss: 0.0216 - val_mean_squared_error: 0.0216\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0183 - mean_squared_error: 0.0183 - val_loss: 0.0209 - val_mean_squared_error: 0.0209\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0177 - mean_squared_error: 0.0177 - val_loss: 0.0203 - val_mean_squared_error: 0.0203\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0197 - val_mean_squared_error: 0.0197\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0192 - val_mean_squared_error: 0.0192\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0186 - val_mean_squared_error: 0.0186\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0155 - mean_squared_error: 0.0155 - val_loss: 0.0181 - val_mean_squared_error: 0.0181\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0150 - mean_squared_error: 0.0150 - val_loss: 0.0177 - val_mean_squared_error: 0.0177\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0146 - mean_squared_error: 0.0146 - val_loss: 0.0173 - val_mean_squared_error: 0.0173\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0142 - mean_squared_error: 0.0142 - val_loss: 0.0170 - val_mean_squared_error: 0.0170\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0138 - mean_squared_error: 0.0138 - val_loss: 0.0165 - val_mean_squared_error: 0.0165\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0133 - mean_squared_error: 0.0133 - val_loss: 0.0161 - val_mean_squared_error: 0.0161\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0130 - mean_squared_error: 0.0130 - val_loss: 0.0158 - val_mean_squared_error: 0.0158\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0126 - mean_squared_error: 0.0126 - val_loss: 0.0153 - val_mean_squared_error: 0.0153\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0123 - mean_squared_error: 0.0123 - val_loss: 0.0149 - val_mean_squared_error: 0.0149\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0120 - mean_squared_error: 0.0120 - val_loss: 0.0147 - val_mean_squared_error: 0.0147\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0117 - mean_squared_error: 0.0117 - val_loss: 0.0144 - val_mean_squared_error: 0.0144\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0114 - mean_squared_error: 0.0114 - val_loss: 0.0139 - val_mean_squared_error: 0.0139\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0111 - mean_squared_error: 0.0111 - val_loss: 0.0136 - val_mean_squared_error: 0.0136\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0109 - mean_squared_error: 0.0109 - val_loss: 0.0133 - val_mean_squared_error: 0.0133\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0106 - mean_squared_error: 0.0106 - val_loss: 0.0131 - val_mean_squared_error: 0.0131\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0104 - mean_squared_error: 0.0104 - val_loss: 0.0128 - val_mean_squared_error: 0.0128\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0101 - mean_squared_error: 0.0101 - val_loss: 0.0126 - val_mean_squared_error: 0.0126\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0099 - mean_squared_error: 0.0099 - val_loss: 0.0124 - val_mean_squared_error: 0.0124\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0097 - mean_squared_error: 0.0097 - val_loss: 0.0121 - val_mean_squared_error: 0.0121\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0094 - mean_squared_error: 0.0094 - val_loss: 0.0118 - val_mean_squared_error: 0.0118\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0092 - mean_squared_error: 0.0092 - val_loss: 0.0116 - val_mean_squared_error: 0.0116\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0090 - mean_squared_error: 0.0090 - val_loss: 0.0114 - val_mean_squared_error: 0.0114\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0089 - mean_squared_error: 0.0089 - val_loss: 0.0112 - val_mean_squared_error: 0.0112\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0087 - mean_squared_error: 0.0087 - val_loss: 0.0110 - val_mean_squared_error: 0.0110\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0085 - mean_squared_error: 0.0085 - val_loss: 0.0108 - val_mean_squared_error: 0.0108\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0083 - mean_squared_error: 0.0083 - val_loss: 0.0105 - val_mean_squared_error: 0.0105\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0082 - mean_squared_error: 0.0082 - val_loss: 0.0104 - val_mean_squared_error: 0.0104\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0080 - mean_squared_error: 0.0080 - val_loss: 0.0102 - val_mean_squared_error: 0.0102\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0079 - mean_squared_error: 0.0079 - val_loss: 0.0101 - val_mean_squared_error: 0.0101\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0077 - mean_squared_error: 0.0077 - val_loss: 0.0100 - val_mean_squared_error: 0.0100\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0076 - mean_squared_error: 0.0076 - val_loss: 0.0097 - val_mean_squared_error: 0.0097\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0074 - mean_squared_error: 0.0074 - val_loss: 0.0095 - val_mean_squared_error: 0.0095\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0073 - mean_squared_error: 0.0073 - val_loss: 0.0093 - val_mean_squared_error: 0.0093\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0072 - mean_squared_error: 0.0072 - val_loss: 0.0092 - val_mean_squared_error: 0.0092\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0070 - mean_squared_error: 0.0070 - val_loss: 0.0090 - val_mean_squared_error: 0.0090\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0069 - mean_squared_error: 0.0069 - val_loss: 0.0088 - val_mean_squared_error: 0.0088\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0068 - mean_squared_error: 0.0068 - val_loss: 0.0087 - val_mean_squared_error: 0.0087\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0067 - mean_squared_error: 0.0067 - val_loss: 0.0086 - val_mean_squared_error: 0.0086\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0066 - mean_squared_error: 0.0066 - val_loss: 0.0084 - val_mean_squared_error: 0.0084\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0064 - mean_squared_error: 0.0064 - val_loss: 0.0082 - val_mean_squared_error: 0.0082\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0063 - mean_squared_error: 0.0063 - val_loss: 0.0080 - val_mean_squared_error: 0.0080\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0062 - mean_squared_error: 0.0062 - val_loss: 0.0079 - val_mean_squared_error: 0.0079\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0061 - mean_squared_error: 0.0061 - val_loss: 0.0078 - val_mean_squared_error: 0.0078\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0060 - mean_squared_error: 0.0060 - val_loss: 0.0076 - val_mean_squared_error: 0.0076\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0059 - mean_squared_error: 0.0059 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0058 - mean_squared_error: 0.0058 - val_loss: 0.0074 - val_mean_squared_error: 0.0074\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0057 - mean_squared_error: 0.0057 - val_loss: 0.0072 - val_mean_squared_error: 0.0072\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0056 - mean_squared_error: 0.0056 - val_loss: 0.0070 - val_mean_squared_error: 0.0070\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0055 - mean_squared_error: 0.0055 - val_loss: 0.0069 - val_mean_squared_error: 0.0069\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0054 - mean_squared_error: 0.0054 - val_loss: 0.0069 - val_mean_squared_error: 0.0069\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0053 - mean_squared_error: 0.0053 - val_loss: 0.0067 - val_mean_squared_error: 0.0067\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0052 - mean_squared_error: 0.0052 - val_loss: 0.0065 - val_mean_squared_error: 0.0065\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0064 - val_mean_squared_error: 0.0064\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0051 - mean_squared_error: 0.0051 - val_loss: 0.0063 - val_mean_squared_error: 0.0063\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0050 - mean_squared_error: 0.0050 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0049 - mean_squared_error: 0.0049 - val_loss: 0.0061 - val_mean_squared_error: 0.0061\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 2ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0060 - val_mean_squared_error: 0.0060\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0048 - mean_squared_error: 0.0048 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 1ms/step - loss: 0.0047 - mean_squared_error: 0.0047 - val_loss: 0.0059 - val_mean_squared_error: 0.0059\n"
     ]
    }
   ],
   "source": [
    "print(\"Fit model on training data\")\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    epochs=100,\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history\n",
    "epochs = history.epoch\n",
    "hist = pd.DataFrame(history.history)\n",
    "mse = hist[\"mean_squared_error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_the_loss_curve(epochs, mse):\n",
    "    \"\"\"Plot a curve of loss vs. epoch.\"\"\"\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Mean Squared Error\")\n",
    "\n",
    "    plt.plot(epochs, mse, label=\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.ylim([mse.min()*0.95, mse.max() * 1.03])\n",
    "    plt.show()  \n",
    "\n",
    "    print(\"Defined the plot_the_loss_curve function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined the plot_the_loss_curve function.\n"
     ]
    }
   ],
   "source": [
    "plot_the_loss_curve(epochs, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000028BDF2F1CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[0.31401446 0.         0.         0.45950034 0.540447   0.63565356\n",
      " 0.61115944 0.53607637]\n",
      "[0.15833333 0.         0.         0.48333333 0.60833333 0.56666667\n",
      " 0.80833333 0.51666667]\n",
      "[0.         0.9177028  0.31202215 0.50239664 0.4934757  0.36134768\n",
      " 0.         0.5702154 ]\n",
      "[0.         0.9        0.43333333 0.625      0.56666667 0.18333333\n",
      " 0.         0.60833333]\n",
      "[0.         0.5332006  0.72556037 0.32545218 0.49615684 0.61015373\n",
      " 0.83832407 0.        ]\n",
      "[0.         0.53333333 0.88333333 0.44166667 0.475      0.59166667\n",
      " 0.875      0.        ]\n",
      "[0.42654175 0.         0.45410335 0.42028156 0.39796433 0.\n",
      " 0.         0.30291238]\n",
      "[0.50833333 0.         0.35833333 0.49166667 0.39166667 0.\n",
      " 0.         0.15833333]\n",
      "[0.33918774 0.58686817 0.58670795 0.56240326 0.41371548 0.\n",
      " 0.01075658 0.        ]\n",
      "[0.43333333 0.63333333 0.54166667 0.65833333 0.475      0.\n",
      " 0.         0.        ]\n",
      "[0.45799    0.5626334  0.         0.5275641  0.22353116 0.\n",
      " 0.00739394 0.46210116]\n",
      "[0.4        0.575      0.         0.525      0.26666667 0.\n",
      " 0.         0.50833333]\n",
      "[0.62626886 0.74583435 0.04837226 0.38085884 0.51208586 0.5727887\n",
      " 0.6256815  0.        ]\n",
      "[0.44166667 0.80833333 0.175      0.53333333 0.56666667 0.46666667\n",
      " 0.45       0.        ]\n",
      "[0.         0.         0.52788484 0.50619805 0.         0.80510575\n",
      " 0.32379726 0.30919433]\n",
      "[0.         0.         0.65       0.56666667 0.         0.76666667\n",
      " 0.275      0.375     ]\n",
      "[0.5213577  0.50107193 0.18571854 0.8323181  0.         0.\n",
      " 0.3388218  0.6208962 ]\n",
      "[0.58333333 0.51666667 0.21666667 0.84166667 0.         0.\n",
      " 0.40833333 0.63333333]\n",
      "[0.45222935 0.         0.6351958  0.5097278  0.50775707 0.6821561\n",
      " 0.3841313  0.6722681 ]\n",
      "[0.50833333 0.         0.58333333 0.475      0.61666667 0.55\n",
      " 0.425      0.6       ]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_train[:10])\n",
    "for i in range(10):\n",
    "    print(predictions[i])\n",
    "    print(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"gradeModel.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\leeji\\AppData\\Local\\Temp\\tmpu41l4plm\\assets\n"
     ]
    }
   ],
   "source": [
    "# Convert the model.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model.\n",
    "with open('gradeModel.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
